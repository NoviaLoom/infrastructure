# Novialoom Core Service

Ce d√©p√¥t contient le service Core de Novialoom, qui fournit une infrastructure LLM (Large Language Model) pour la g√©n√©ration de texte.

## üìÇ Structure du D√©p√¥t

*   **`services/core-service/`** : Service Core - Infrastructure LLM avec support Google Gemini et OpenAI
*   **`shared/`** : Code partag√© (Mod√®les Pydantic, Auth, Configuration, Health checks, Logging)

## üöÄ Core Service

Le Core Service fournit une interface unifi√©e pour l'utilisation de diff√©rents providers LLM :
- **Google Gemini** (mod√®les 2.5) avec Google Search et Google Maps grounding
- **OpenAI** (GPT-4, GPT-3.5-turbo)
- **Embeddings** : Service d'embeddings pour la vectorisation de texte

Voir [services/core-service/README.md](services/core-service/README.md) pour plus de d√©tails.

## ‚ö° Architecture Serverless

Le service peut √™tre packag√© dans un container Docker et expos√© via AWS Lambda gr√¢ce √† **Mangum**.

### Comment ajouter Mangum au service ?
1.  Ajouter `mangum` dans le `requirements.txt`.
2.  Dans le `main.py` :
    ```python
    from mangum import Mangum
    from fastapi import FastAPI
    app = FastAPI()
    handler = Mangum(app, lifespan="off")
    ```

## üõ†Ô∏è D√©veloppement Local

Voir la documentation dans `services/core-service/README.md` pour les instructions de d√©veloppement local.

---
*Derni√®re mise √† jour : Janvier 2026*

